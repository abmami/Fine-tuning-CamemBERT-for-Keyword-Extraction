{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finetuning CamemBERT on Keywords Extraction Task using Custom Dataset and WikiNews French Keywords Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we will see how to fine-tune CamemBERT on a Keywords Extraction task. We will use the CamemBERT model, fine-tuned on the Semantic Similarity task, as a starting point, then we will fine-tune it on the Keywords Extraction task. Alternatively, we will also see how to fine-tune CamemBERT on the Keywords Extraction task from scratch. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SYfTpanjLFaq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/abdessalem/Desktop/Projects/edu-keywords-extraction/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForTokenClassification\n",
        "import lightning.pytorch as pl\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "import json\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "# Load config json\n",
        "root_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
        "os.chdir(root_dir)\n",
        "\n",
        "with open(\"config.json\") as f:\n",
        "    config = json.load(f)\n",
        "    \n",
        "data_config = config['data']\n",
        "dataset = os.path.join(data_config['data_folder'], data_config['KE_DATASET']['name'])\n",
        "\n",
        "data = {\n",
        "    \"train\": os.path.join(dataset, \"train.csv\"),\n",
        "    \"val\": os.path.join(dataset, \"val.csv\"),\n",
        "    \"test\": os.path.join(dataset, \"test.csv\"),\n",
        "}\n",
        "\n",
        "def preprocess(data):\n",
        "    data = data.dropna()\n",
        "    data = data[data['keywords'] != '']\n",
        "    data = data.drop_duplicates(subset=['keywords'])\n",
        "    data['keywords'] = data['keywords'].apply(lambda x: x.replace('[', '').replace(']', '').replace(\"'\", '').replace(',', '\\t'))\n",
        "    data['keywords'] = data['keywords'].apply(lambda x: x[:-1] if x[-1] == ' ' else x)\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "model_name = \"camembert-base\"\n",
        "max_length = 256\n",
        "#\"camembert/camembert-large\"\n",
        "# \"yanekyuk/camembert-keyword-extractor\"\n",
        "\n",
        "\n",
        "class MyDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, train_csv, val_csv, test_csv, batch_size):\n",
        "        super().__init__()\n",
        "        self.train_csv = train_csv\n",
        "        self.val_csv = val_csv\n",
        "        self.test_csv = test_csv\n",
        "        self.batch_size = batch_size\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_data = MyDataset(self.train_csv, self.tokenizer)\n",
        "        self.val_data = MyDataset(self.val_csv, self.tokenizer)\n",
        "        self.test_data = MyDataset(self.test_csv, self.tokenizer)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_data, batch_size=self.batch_size)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test_data, batch_size=self.batch_size)\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer):\n",
        "        self.data = preprocess(pd.read_csv(csv_file))\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label2id = {\"B-KEY\": 1, \"I-KEY\": 2, \"O\": 0}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.data.iloc[index]['text']\n",
        "        keywords = self.data.iloc[index]['keywords'].split('\\t')\n",
        "        \n",
        "        tokens = self.tokenizer.tokenize(text,truncation=True, padding=True, max_length=max_length)\n",
        "        labels = [\"O\"] * len(tokens)\n",
        "\n",
        "        for keyword in keywords:\n",
        "            keyword_tokens = self.tokenizer.tokenize(keyword,truncation=True, padding=True, max_length=max_length)\n",
        "            # find the start index of the keyword in the tokens\n",
        "            keyword_start_idx = -1\n",
        "            for i in range(len(tokens) - len(keyword_tokens) + 1):\n",
        "                if tokens[i:i+len(keyword_tokens)] == keyword_tokens:\n",
        "                    keyword_start_idx = i\n",
        "                    break\n",
        "            if keyword_start_idx >= 0:\n",
        "                labels[keyword_start_idx] = \"B-KEY\"\n",
        "                for i in range(keyword_start_idx+1, keyword_start_idx+len(keyword_tokens)):\n",
        "                    labels[i] = \"I-KEY\"\n",
        "\n",
        "        label_ids = [self.label2id[label] for label in labels]\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        padding_length = max_length - len(input_ids)\n",
        "\n",
        "        input_ids = input_ids + ([0] * padding_length)\n",
        "        attention_mask = attention_mask + ([0] * padding_length)\n",
        "        label_ids = label_ids + ([0] * padding_length)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(label_ids, dtype=torch.long)\n",
        "        }\n",
        "    \n",
        "\n",
        "# Create dataloaders\n",
        "\n",
        "batch_size = 4\n",
        "dm = MyDataModule(data['train'], data['val'], data['test'], batch_size)\n",
        "dm.setup()\n",
        "\n",
        "# Retrieve a batch of data\n",
        "batch = next(iter(dm.train_dataloader()))\n",
        "print(batch['input_ids'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                            | Params\n",
            "----------------------------------------------------------\n",
            "0 | model | CamembertForTokenClassification | 110 M \n",
            "----------------------------------------------------------\n",
            "110 M     Trainable params\n",
            "0         Non-trainable params\n",
            "110 M     Total params\n",
            "440.135   Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/abdessalem/Desktop/Projects/edu-keywords-extraction/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/abdessalem/Desktop/Projects/edu-keywords-extraction/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:  10%|█         | 185/1806 [00:52<07:40,  3.52it/s, v_num=1]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/abdessalem/Desktop/Projects/edu-keywords-extraction/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/home/abdessalem/Desktop/Projects/edu-keywords-extraction/venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0: 100%|██████████| 227/227 [00:25<00:00,  9.01it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9807493090629578     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3410547851214217     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.30427297949790955    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9807493090629578    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3410547851214217    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.30427297949790955   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[{'test_loss': 0.30427297949790955,\n",
              "  'test_acc': 0.9807493090629578,\n",
              "  'test_f1': 0.3410547851214217}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class MyModel(pl.LightningModule):\n",
        "    def __init__(self, num_labels):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.num_labels = num_labels\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)       \n",
        "        self.model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=self.num_labels)\n",
        "        \n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels):\n",
        "        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(batch['input_ids'], batch['attention_mask'], batch['labels'])\n",
        "        loss = outputs.loss\n",
        "        self.log('train_loss', loss)\n",
        "        acc = (outputs.logits.argmax(-1) == batch['labels']).float().mean()\n",
        "        self.log('train_acc', acc)\n",
        "        f1 = f1_score(batch['labels'].cpu().numpy().flatten(), outputs.logits.argmax(-1).cpu().numpy().flatten(), average='macro')\n",
        "        self.log('train_f1', f1)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(batch['input_ids'], batch['attention_mask'], batch['labels'])\n",
        "        loss = outputs.loss\n",
        "        acc = (outputs.logits.argmax(-1) == batch['labels']).float().mean()\n",
        "        f1 = f1_score(batch['labels'].cpu().numpy().flatten(), outputs.logits.argmax(-1).cpu().numpy().flatten(), average='macro')\n",
        "        self.log('val_loss', loss)\n",
        "        self.log('val_acc', acc)\n",
        "        self.log('val_f1', f1)\n",
        "        \n",
        "    \n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        outputs = self.forward(batch['input_ids'], batch['attention_mask'], batch['labels'])\n",
        "        loss = outputs.loss\n",
        "        self.log('test_loss', loss)\n",
        "        acc = (outputs.logits.argmax(-1) == batch['labels']).float().mean()\n",
        "        self.log('test_acc', acc)\n",
        "        f1 = f1_score(batch['labels'].cpu().numpy().flatten(), outputs.logits.argmax(-1).cpu().numpy().flatten(), average='macro')\n",
        "        self.log('test_f1', f1)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        #return AdamW(self.parameters(), lr=5e-5)\n",
        "        return Adam(self.parameters(), lr=2e-5, eps=1e-08, betas=(0.9, 0.999))\n",
        "    \n",
        "    def infer(self, text):\n",
        "        tokens = self.tokenizer.tokenize(text,truncation=True, padding=True, max_length=max_length)\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        outputs = self.model(torch.tensor([input_ids]), torch.tensor([attention_mask]))\n",
        "        # return keyword tokens and labels\n",
        "        return outputs.logits.argmax(-1)\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "# Create model\n",
        "model = MyModel(num_labels=3)\n",
        "\n",
        "# Create trainer \n",
        "\n",
        "trainer = pl.Trainer(accelerator='auto', max_epochs=7, devices=[0], accumulate_grad_batches=8)\n",
        "\n",
        "# Train model\n",
        "\n",
        "trainer.fit(model, dm)\n",
        "\n",
        "# Test model\n",
        "\n",
        "trainer.test(model, datamodule=dm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def extract_keywords(text, model):\n",
        "    # use infer method to get the labels\n",
        "    labels = model.infer(text)[0]\n",
        "    # get the tokens from the text\n",
        "    tokens = model.tokenizer.tokenize(text,truncation=True, padding=True, max_length=max_length)\n",
        "    # get the keywords from the tokens and labels\n",
        "    keywords = []\n",
        "    for i in range(len(tokens)):\n",
        "        if labels[i] == 1:\n",
        "            keyword = tokens[i]\n",
        "            for j in range(i+1, len(tokens)):\n",
        "                if labels[j] == 2:\n",
        "                    keyword += \" \" + tokens[j]\n",
        "                else:\n",
        "                    break\n",
        "            # convert the keyword to the original string\n",
        "            keyword = model.tokenizer.convert_tokens_to_string(model.tokenizer.tokenize(keyword))\n",
        "            keywords.append(keyword)\n",
        "\n",
        "    return keywords\n",
        "\n",
        "\n",
        "text2 = \"Je vous avais préparé encore un autre exercice pour s'habituer un petit peu avec les techniques d'enregistrement comptable des opérations commerciales. Vous avez ici un bilan initial, construction 100 mèles euros, client 15 mèles euros, banque 13 mèles euros, caisse 2 mèles euros, au total 130 mèles euros, capital 100 mèles euros, fournisseur 30 mèles euros, total la même chose. Voilà, on a ici un bilan initial, tout simple, et on va essayer de voir un petit peu ce qui est demandé dans l'exercice. Donc ici vous avez les opérations, les opérations que l'entreprise a effectuées, et tout en bas on a un travail à faire. Le travail à faire c'est quoi ? On va effectuer l'ouverture des comptes, on va enregistrer les opérations dans les comptes en T, et puis on va calculer le résultat de l'entreprise, on va établir la balance définitive, et au final établir le bilan final. J'espère bien arriver à un bilan final équilibré. J'essaierai de me concentrer pour que je ne fasse pas d'erreurs, parce que dans ce genre d'exercice, une simple erreur quelque part peut aboutir à un bilan final déséquilibré, c'est-à-dire total actif et différent de total passif.\"\n",
        "\n",
        "extract_keywords(text2, model)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
