{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-finetuning CamemBERT on Sentence Similarity Task using PAWS-C french dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will pre-finetune CamemBERT on Sentences Similarity task using PAWS-C french dataset. The goal is to pre-finetune CamemBERT on a french dataset before fine-tuning it on French keywords extraction task. We'll use the PyTorch Lightning framework to train the model, and the HuggingFace Transformers library to load the model and tokenizer. This notebook is just for testing purposes, we'll use the script run_task.py to pre-finetune the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch transformers lightning datasets seaborn plotly pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "pl.trainer.seed_everything(42)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config json\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "os.chdir(root_dir)\n",
    "\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "data_config = config['data']\n",
    "dataset = os.path.join(data_config['data_folder'], data_config['SS_DATASET']['name'])\n",
    "data = {\n",
    "    \"train\": os.path.join(dataset, \"translated_train.tsv\"),\n",
    "    \"dev\": os.path.join(dataset, \"dev_2k.tsv\"),\n",
    "    \"test\": os.path.join(dataset, \"test_2k.tsv\")\n",
    "}\n",
    "\n",
    "\n",
    "def load_process():\n",
    "    train = pd.read_csv(data['train'], delimiter='\\t', on_bad_lines='skip')\n",
    "    dev = pd.read_csv(data['dev'], delimiter='\\t', on_bad_lines='skip')\n",
    "    test = pd.read_csv (data['test'], delimiter='\\t', on_bad_lines='skip')\n",
    "\n",
    "    train.drop(columns=['id'], inplace=True)\n",
    "    dev.drop(columns=['id'], inplace=True)\n",
    "    test.drop(columns=['id'], inplace=True)\n",
    "\n",
    "    train.dropna(inplace=True)\n",
    "    dev.dropna(inplace=True)\n",
    "    test.dropna(inplace=True)\n",
    "\n",
    "    train['label'] = train['label'].astype(int)\n",
    "    dev['label'] = dev['label'].astype(int)\n",
    "    test['label'] = test['label'].astype(int)\n",
    "\n",
    "    return train, dev, test\n",
    "\n",
    "\n",
    "train, dev, test = load_process()\n",
    "\n",
    "# Create PyTorch datasets from the dataframes\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "dev_dataset = Dataset.from_pandas(dev)\n",
    "test_dataset = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train samples : 49127\n",
      "Total validation samples: 1988\n",
      "Total test samples: 2000\n"
     ]
    }
   ],
   "source": [
    "# Shape of the data\n",
    "\n",
    "print(f\"Total train samples : {train.shape[0]}\")\n",
    "print(f\"Total validation samples: {dev.shape[0]}\")\n",
    "print(f\"Total test samples: {test.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
    "batch_size = 16\n",
    "\n",
    "def tokenize_batch(samples, tokenizer):\n",
    "    sentence_1 = [sample['sentence1'] for sample in samples]\n",
    "    sentence_2 = [sample['sentence2'] for sample in samples]\n",
    "    labels = torch.tensor([sample[\"label\"] for sample in samples])\n",
    "    str_labels = [sample[\"label\"] for sample in samples]\n",
    "    text = [[str(x), str(y)] for x,y in zip(sentence_1, sentence_2)]\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", padding='max_length', max_length = 128, truncation=True)\n",
    "\n",
    "    return {\"input_ids\": tokens.input_ids, \"attention_mask\": tokens.attention_mask, \"labels\": labels, \"str_labels\": str_labels, \"sentences\": text}\n",
    "\n",
    "# Create dataloaders \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    pin_memory=True,\n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer)\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dev_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    pin_memory=True,\n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    pin_memory=True,\n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "lr = 3e-5\n",
    "weight_decay = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_labels, lr, weight_decay, from_scratch=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        if from_scratch:\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                model_name, num_labels=num_labels\n",
    "            ).to(\"cuda\")\n",
    "            self.model = AutoModelForSequenceClassification.from_config(config)\n",
    "        else:\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_name, num_labels=num_labels\n",
    "            ).to(\"cuda\")\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_labels = self.model.num_labels\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        out = self.forward(batch)\n",
    "        logits = out.logits\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits.view(-1, self.num_labels), batch[\"labels\"].view(-1))\n",
    "        self.log(\"train/loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        labels = batch[\"labels\"]\n",
    "        out = self.forward(batch)\n",
    "        preds = torch.max(out.logits, -1).indices\n",
    "        acc = (batch[\"labels\"] == preds).float().mean()\n",
    "        self.log(\"valid/acc\", acc)\n",
    "        # If you’re trying to clear up the attached computational graph, use .detach() instead.\n",
    "        f1 = f1_score(labels.detach().cpu().numpy(), preds.detach().cpu().numpy(), average='macro')\n",
    "        self.log(\"valid/f1\", f1)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        return torch.max(out.logits, -1).indices\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/abdessalem/Desktop/Projects/edu-keywords-extraction/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                               | Params\n",
      "-------------------------------------------------------------\n",
      "0 | model | CamembertForSequenceClassification | 110 M \n",
      "-------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.494   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  61%|██████    | 1875/3071 [19:59<12:44,  1.56it/s, v_num=0]      "
     ]
    }
   ],
   "source": [
    "lightning_model = LightningModel(\"camembert-base\", 2, lr=lr, weight_decay=weight_decay)\n",
    "model_checkpoint = pl.callbacks.ModelCheckpoint(monitor=\"valid/acc\", mode=\"max\")\n",
    "\n",
    "camembert_trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    #precision=16, \n",
    "    accelerator=\"gpu\", devices=\"auto\",\n",
    "    callbacks=[\n",
    "        pl.callbacks.EarlyStopping(monitor=\"valid/acc\", patience=4, mode=\"max\"),\n",
    "        model_checkpoint,\n",
    "    ]\n",
    ")\n",
    "camembert_trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model,trainer,test_set):\n",
    "    preds = trainer.predict(model,dataloaders=test_set)\n",
    "    # does trainer.predict do a forward pass on the model? > yes\n",
    "    # does it change the model weights? > no\n",
    "    preds = torch.cat(preds, -1)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    preds = preds.tolist()\n",
    "    test['preds'] = preds\n",
    "    test['preds'] = test['preds'].astype(int)\n",
    "    test['label'] = test['label'].astype(int)\n",
    "    print(f\"Accuracy: {sum(test['preds'] == test['label'])/len(test)}\")\n",
    "    print(f\"F1 score: {f1_score(test['preds'], test['label'], average='macro')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on dev and test sets\n",
    "\n",
    "print(\"Dev set\")\n",
    "#eval(lightning_model, camembert_trainer, val_dataloader)\n",
    "\n",
    "print(\"Test set\")\n",
    "eval(lightning_model, camembert_trainer, test_dataloader)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
