{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torch transformers sentencepiece pytorch_lightning datasets tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM6uXD5fpONz",
        "outputId": "3c92ed06-9a37-44a3-82a7-fbb50e71f22d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.4.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.11.4)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.8.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SYfTpanjLFaq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from transformers import CamembertTokenizer, AdamW, CamembertForSequenceClassification, CamembertConfig, CamembertForTokenClassification\n",
        "import pandas as pd\n",
        "from transformers import CamembertForTokenClassification, CamembertConfig, AutoModelForTokenClassification, CamembertPreTrainedModel\n",
        "from pprint import pprint\n",
        "import functools\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from transformers import AutoModelForSequenceClassification, CamembertForMaskedLM, AutoTokenizer, AutoConfig\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "data = pd.read_csv(\"final-keys-dataset.csv\")\n",
        "\n",
        "# Drop the rows with missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Remove the rows with empty strings\n",
        "\n",
        "data = data[data['keywords'] != '']\n",
        "\n",
        "# Remove duplicates\n",
        "\n",
        "data = data.drop_duplicates(subset=['keywords'])\n",
        "\n",
        "# Convert the keywords into a string of words separated by spaces instead of a list\n",
        "\n",
        "data['keywords'] = data['keywords'].apply(lambda x: ' '.join(x.split(',')))\n",
        "\n",
        "# Remove the brackets\n",
        "\n",
        "data['keywords'] = data['keywords'].apply(lambda x: x.replace('[', '').replace(']', ''))\n",
        "\n",
        "# Remove the quotes\n",
        "\n",
        "data['keywords'] = data['keywords'].apply(lambda x: x.replace(\"'\", ''))\n",
        "\n",
        "# Get only 6 keywords, if there are more than 6. If there are less than 6, drop the row\n",
        "\n",
        "data['keywords'] = data['keywords'].apply(lambda x: ' '.join(x.split()[:6]))\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"yanekyuk/camembert-keyword-extractor\")\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"yanekyuk/camembert-keyword-extractor\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nMQMv9Q8LFat"
      },
      "outputs": [],
      "source": [
        "# Create a list of dictionaries where each dictionary represents a document and its corresponding keywords\n",
        "datax = []\n",
        "for index, row in data.iterrows():\n",
        "    doc = {\n",
        "        \"text\": row[\"text\"],\n",
        "        \"keywords\": row['keywords'].split()\n",
        "    }\n",
        "    datax.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(datax[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvSY0FvqqpUc",
        "outputId": "34020aae-09cf-458c-9fc7-8e7ecade6cf8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"Pour bien maîtriser la logique de l'enregistrement des opérations comptables, on va faire cet exercice. On a un bilan initial ici et on a plusieurs opérations à enregistrer. Je vais ajouter ici le compte de résultats parce qu'on va l'utiliser pendant l'enregistrement comptable des opérations. En fait, j'ai déjà répondu à ce même exercice en utilisant les schémas en T. Dans cet exercice-là, cette fois-ci, on va utiliser simplement l'impact qu'il y aurait sur le bilan initial et le compte de résultats directement sans passer par les schémas en T. On va garder cette image pour vérification après. Donc, on devrait arriver au même bilan final qu'ici. On commence tout de suite par la première opération, achat de marchandise 2300 € hors TVA. Le taux de TVA est de 21 %. Dans cette opération, on a une augmentation de charge au niveau achat de marchandise. Donc, on va augmenter le compte achat de marchandise de 2300 € et on reçoit une facture à la suite de cet achat du fournisseur. Eh bien, on va augmenter la dette fournisseur de la même somme car la facture que je vais payer au fournisseur inclut la TVA. C'est-à-dire que je vais payer au fournisseur les marchandises plus la TVA appliquée sur ces marchandises. Mais cette TVA que je paye au fournisseur, je vais la récupérer de l'état. Donc, pour moi, c'est une créance que je vais recevoir plus tard de l'état. Ça doit figurer au bilan, ici, comme TVA a récupéré. Au bien TVA sur achat. Je vais l'ajouter ici, TVA sur achat 483. Je passe à l'opération numéro 2, paiement de la facture numéro 1 par la banque. La facture numéro 1, elle est ici, ça concerne l'achat des marchandises. Donc, on va payer cette facture-là au fournisseur. Et donc, ce qui va se passer, c'est qu'on va réduire cette dette fournisseur de la somme de la facture payée, moins 2300, moins 483. Et dans le même temps, on va avoir la même diminution au niveau de notre compte bancaire puisque on paye par la banque. Je passe à l'opération 3, vente de marchandises achetées à 3500 euros. TVA, 21%. Dans cette opération, on réalise un produit. Vente de marchandises va augmenter de la somme de 3500 au produit. Et au même temps, puisque on envoie une facture à un client, celui qui a acheté les marchandises, eh bien les créances clients vont augmenter de la même somme plus la TVA appliquée sur cette même somme, 21%. Ce qui donnerait 735 euros. Mais cette TVA que le client va me payer, en fait, c'est une dette envers l'État. Je vais la transférer dans les caisses de l'État. Ça reste une dette que je dois encoder au passif comme dette à payer, comme TVA à payer, à l'État, bien sûr. Cette TVA, on l'appelle aussi TVA sur vente. Je passe à la quatrième question. On reçoit une facture d'électricité 150 euros. 6% de TVA en plus. On va avoir donc une augmentation des charges au niveau services et bien diverses, le compte, l'électricité. Et puisque il y a de la TVA dessus, donc cette TVA que je vais payer, je vais la récupérer de l'État, donc ça va faire 9 euros de TVA que je vais ajouter à TVA sur achat. Puis, puisque je reçois une facture d'un fournisseur dans laquelle figure aussi bien l'achat ou bien l'électricité de 150 euros plus la TVA de 9 euros, j'augmente mon fournisseur de ces deux sommes. Je passe à la dernière opération, paiement de la facture d'électricité par la banque. Alors ce qui va se passer, c'est que ma banque va diminuer de la somme de la facture, la facture totale 150 plus 9 euros et le fournisseur va se réduire également de la même somme. Et voilà, mes opérations sont enregistrées. Ce qui reste à faire maintenant, c'est de calculer le bénéfice. Et le bénéfice, c'est simple, on fait les produits de 3500, moins les charges qui sont de l'ordre de 2300 plus 150 euros. Ce qui donne un bénéfice de 1050 euros. Et ce bénéfice va s'ajouter à notre bilan final comme une nouvelle ressource ici à côté du capital. On va ajouter donc à notre bilan bénéfice reporté. On va voir maintenant si on arrive au même résultat que avec les schémas en T. On a ici le bilan final et on va juste comparer le résultat final. Je commence par le passif. On a ici un capital de 60400 et là on a 60000. En fait, c'est une erreur. Le capital, on ne l'a pas touché. C'est juste une erreur en fait. J'aurais dû mettre ici 60400 dans l'exercice. Alors le bénéfice, 1050, 1050, fournisseur, ça reste à 1300. Donc ici on voit que 150, ça part avec 150, 9 avec 9 et 2300 avec 2300. 483 avec 483. TVA sur vente, ce qu'on appelle TVA à payer, c'est 735, 735. Et bien au passif, c'est tout juste. Au niveau actif, qu'est-ce qui est changé ? On a les clients, on devrait avoir 9435. Et si je fais le calcul, je trouve exactement la même chose. TVA à récupérer, TVA sur achat, c'est 483 plus 9, ça fait la même chose. La banque, on doit avoir 12058. Et si je fais le calcul de tous là, je vais tomber sur 12058. La caisse, on n'a pas touché, ça reste à 500 euros. Et donc on arrive au même résultat avec les chemins hantés et en faisant juste les mouvements sur le bilan directement.\", 'keywords': ['mouvements', 'Exercice', 'Education', 'Opérations', 'bilan', 'compte']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, label2id):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label2id = label2id\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        doc = self.data[index]\n",
        "        text = str(doc[\"text\"])\n",
        "        text = text[:300]\n",
        "        tokens = self.tokenizer.tokenize(text, truncation=True, padding=True, max_length=128*2)\n",
        "\n",
        "        labels = [\"O\"] * len(tokens)\n",
        "\n",
        "        for keyword in doc[\"keywords\"]:\n",
        "            keyword_tokens = self.tokenizer.tokenize(keyword,truncation=True, padding=True, max_length=128*2)\n",
        "            keyword_start_idx = -1\n",
        "            for i in range(len(tokens) - len(keyword_tokens) + 1):\n",
        "                if tokens[i:i+len(keyword_tokens)] == keyword_tokens:\n",
        "                    keyword_start_idx = i\n",
        "                    break\n",
        "            if keyword_start_idx >= 0:\n",
        "                labels[keyword_start_idx] = \"B-KEY\"\n",
        "                for i in range(keyword_start_idx+1, keyword_start_idx+len(keyword_tokens)):\n",
        "                    labels[i] = \"I-KEY\"\n",
        "\n",
        "        label_ids = [self.label2id[label] for label in labels]\n",
        "\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(label_ids)\n"
      ],
      "metadata": {
        "id": "429hQK6OibkE"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids, attention_masks, labels = zip(*batch)\n",
        "    max_len = max(len(ids) for ids in input_ids)\n",
        "    input_ids_tensor = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "    attention_masks_tensor = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "    labels_tensor = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
        "    for i, ids in enumerate(input_ids):\n",
        "        input_ids_tensor[i, :len(ids)] = torch.tensor(ids)\n",
        "        attention_masks_tensor[i, :len(ids)] = torch.ones(len(ids), dtype=torch.long)\n",
        "        labels_tensor[i, :len(ids)] = torch.tensor(labels[i])\n",
        "    input_ids_tensor = torch.nn.utils.rnn.pad_sequence(input_ids_tensor, batch_first=True, padding_value=-100)\n",
        "    labels_tensor = torch.nn.utils.rnn.pad_sequence(labels_tensor, batch_first=True, padding_value=-100)\n",
        "    return input_ids_tensor, attention_masks_tensor, labels_tensor\n",
        "\n",
        "\n",
        "dataset = TokenClassificationDataset(datax, tokenizer,{\"B-KEY\": 1, \"I-KEY\": 2, \"O\": 0})\n",
        "# Split the dataset into train and validation sets\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create the data loaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "#dataset = TokenClassificationDataset(datax, tokenizer,{\"B-KEY\": 1, \"I-KEY\": 2, \"O\": 0})\n",
        "#dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "lr384f25ivwZ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in val_dataloader:\n",
        "  print(d[0].shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gE7o494i9CJ",
        "outputId": "d7c5fa3d-9bc2-468e-8a9d-316cc1ac65e9"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 120])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-e9846bbc52d0>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_ids_tensor[i, :len(ids)] = torch.tensor(ids)\n",
            "<ipython-input-60-e9846bbc52d0>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels_tensor[i, :len(ids)] = torch.tensor(labels[i])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46yhnluPLFav",
        "outputId": "5c7db8b4-98b6-4dfc-ebc7-cefad244964a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-74-f54ee28d1cc3>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_ids_tensor[i, :len(ids)] = torch.tensor(ids)\n",
            "<ipython-input-74-f54ee28d1cc3>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels_tensor[i, :len(ids)] = torch.tensor(labels[i])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.10460487699234172\n",
            "Training accuracy: 0.9780465960502625\n",
            "Validation loss: 0.06466724053025245\n",
            "Validation accuracy: 0.9872932434082031\n",
            "Epoch 2/20\n",
            "----------\n",
            "Training loss: 0.0581430382781515\n",
            "Training accuracy: 0.9832212328910828\n",
            "Validation loss: 0.05101604908704758\n",
            "Validation accuracy: 0.9869446754455566\n",
            "Epoch 3/20\n",
            "----------\n",
            "Training loss: 0.036226922229520585\n",
            "Training accuracy: 0.9870401620864868\n",
            "Validation loss: 0.059096026420593264\n",
            "Validation accuracy: 0.9816984534263611\n",
            "Epoch 4/20\n",
            "----------\n",
            "Training loss: 0.019514814631915407\n",
            "Training accuracy: 0.9943149089813232\n",
            "Validation loss: 0.07239570170640945\n",
            "Validation accuracy: 0.9869599342346191\n",
            "Epoch 5/20\n",
            "----------\n",
            "Training loss: 0.010728703894480867\n",
            "Training accuracy: 0.9974284172058105\n",
            "Validation loss: 0.07245907820761203\n",
            "Validation accuracy: 0.9859991073608398\n",
            "Epoch 6/20\n",
            "----------\n",
            "Training loss: 0.007080836448279258\n",
            "Training accuracy: 0.9985268115997314\n",
            "Validation loss: 0.07585513181984424\n",
            "Validation accuracy: 0.9843633770942688\n",
            "Epoch 7/20\n",
            "----------\n",
            "Training loss: 0.0045646310808431165\n",
            "Training accuracy: 0.99908846616745\n",
            "Validation loss: 0.07926750481128693\n",
            "Validation accuracy: 0.984973132610321\n",
            "Epoch 8/20\n",
            "----------\n",
            "Training loss: 0.003468109625636747\n",
            "Training accuracy: 0.9994131326675415\n",
            "Validation loss: 0.08152463287115097\n",
            "Validation accuracy: 0.9843304753303528\n",
            "Epoch 9/20\n",
            "----------\n",
            "Training loss: 0.002767454209357598\n",
            "Training accuracy: 0.9994706511497498\n",
            "Validation loss: 0.08908758834004402\n",
            "Validation accuracy: 0.9846397638320923\n",
            "Epoch 10/20\n",
            "----------\n",
            "Training loss: 0.002290890066529085\n",
            "Training accuracy: 0.9996927976608276\n",
            "Validation loss: 0.09265082404017448\n",
            "Validation accuracy: 0.9856113791465759\n",
            "Epoch 11/20\n",
            "----------\n",
            "Training loss: 0.0028858363833646045\n",
            "Training accuracy: 0.9994497895240784\n",
            "Validation loss: 0.09081193283200265\n",
            "Validation accuracy: 0.9843956828117371\n",
            "Epoch 12/20\n",
            "----------\n",
            "Training loss: 0.003729783035324592\n",
            "Training accuracy: 0.9992539286613464\n",
            "Validation loss: 0.08199940621852875\n",
            "Validation accuracy: 0.985916256904602\n",
            "Epoch 13/20\n",
            "----------\n",
            "Training loss: 0.0026088776970649825\n",
            "Training accuracy: 0.9995676279067993\n",
            "Validation loss: 0.0879758432507515\n",
            "Validation accuracy: 0.9843626022338867\n",
            "Epoch 14/20\n",
            "----------\n",
            "Training loss: 0.0025742571459360128\n",
            "Training accuracy: 0.9995523691177368\n",
            "Validation loss: 0.09419061653316022\n",
            "Validation accuracy: 0.9865874648094177\n",
            "Epoch 15/20\n",
            "----------\n",
            "Training loss: 0.003395936321650975\n",
            "Training accuracy: 0.9993847608566284\n",
            "Validation loss: 0.08481529280543328\n",
            "Validation accuracy: 0.985639750957489\n",
            "Epoch 16/20\n",
            "----------\n",
            "Training loss: 0.003486132102951064\n",
            "Training accuracy: 0.9991891980171204\n",
            "Validation loss: 0.0858697310090065\n",
            "Validation accuracy: 0.9859554171562195\n",
            "Epoch 17/20\n",
            "----------\n",
            "Training loss: 0.003283706479889684\n",
            "Training accuracy: 0.9993866682052612\n",
            "Validation loss: 0.08377452678978443\n",
            "Validation accuracy: 0.9839839339256287\n",
            "Epoch 18/20\n",
            "----------\n",
            "Training loss: 0.0022748173528510194\n",
            "Training accuracy: 0.9995085000991821\n",
            "Validation loss: 0.09453790038824081\n",
            "Validation accuracy: 0.9840230941772461\n",
            "Epoch 19/20\n",
            "----------\n",
            "Training loss: 0.0021766664469492084\n",
            "Training accuracy: 0.9996100068092346\n",
            "Validation loss: 0.09453667402267456\n",
            "Validation accuracy: 0.9856221079826355\n",
            "Epoch 20/20\n",
            "----------\n",
            "Training loss: 0.001669629953348225\n",
            "Training accuracy: 0.9996861219406128\n",
            "Validation loss: 0.09359265714883805\n",
            "Validation accuracy: 0.9859554171562195\n"
          ]
        }
      ],
      "source": [
        "# Define the model architecture using pytorch-lightning\n",
        "\n",
        "import transformers\n",
        "from transformers import CamembertForTokenClassification, CamembertConfig, AutoModelForTokenClassification, CamembertPreTrainedModel\n",
        "from transformers.utils import ModelOutput\n",
        "\n",
        "label2id = {\n",
        "    \"B-KEY\": 1,\n",
        "    \"I-KEY\": 2,\n",
        "    \"O\": 0\n",
        "  }\n",
        "class CamembertForTokenClassification2(CamembertPreTrainedModel):\n",
        "\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        config = CamembertConfig.from_pretrained(\"yanekyuk/camembert-keyword-extractor\")\n",
        "        self.config = config\n",
        "        self.config.num_labels = len(label2id)\n",
        "\n",
        "        self.model = AutoModelForTokenClassification.from_pretrained(\"yanekyuk/camembert-keyword-extractor\", config=self.config)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = torch.nn.Dropout(classifier_dropout)\n",
        "        self.classifier = torch.nn.Linear(config.hidden_size,config.num_labels)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(self,input_ids = None,attention_mask = None, labels= None):\n",
        "        \"\"\"\n",
        "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        #logits = self.classifier(sequence_output)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            # move labels to correct device to enable model parallelism\n",
        "            labels = labels.to(logits.device)\n",
        "            loss_fct = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, 3), labels.view(-1))\n",
        "\n",
        "\n",
        "        return transformers.modeling_outputs.TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "# Define the model\n",
        "device = torch.device('cuda')\n",
        "\n",
        "config = CamembertConfig.from_pretrained(\"yanekyuk/camembert-keyword-extractor\")\n",
        "model = CamembertForTokenClassification2(config=config)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "def flat_accuracy(logits, labels):\n",
        "    # Flatten the predictions and true values to 1 dimension\n",
        "    flat_logits = torch.argmax(logits, dim=2).flatten()\n",
        "    flat_labels = labels.flatten()\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    return (flat_logits == flat_labels).float().mean()\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    train_loss, train_accuracy = 0, 0\n",
        "    nb_train_steps, nb_train_examples = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            print(f'Batch {step} of {len(train_dataloader)}')\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        outputs = model(input_ids=b_input_ids, \n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        if loss is not None:\n",
        "          train_loss += loss.item()\n",
        "          loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        nb_train_examples += b_input_ids.size(0)\n",
        "        nb_train_steps += 1\n",
        "\n",
        "        train_accuracy += flat_accuracy(logits, b_labels)\n",
        "\n",
        "    print(f'Training loss: {train_loss / nb_train_steps}')\n",
        "    print(f'Training accuracy: {train_accuracy / nb_train_steps}')\n",
        "\n",
        "    test_loss, test_accuracy = 0, 0\n",
        "    nb_test_steps, nb_test_examples = 0, 0\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=b_input_ids, \n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        if loss is not None:\n",
        "          test_loss += loss.item()\n",
        "\n",
        "        test_accuracy += flat_accuracy(logits, b_labels)\n",
        "\n",
        "\n",
        "        nb_test_examples += b_input_ids.size(0)\n",
        "        nb_test_steps += 1\n",
        "\n",
        "    print(f'Validation loss: {test_loss / nb_test_steps}')\n",
        "    print(f'Validation accuracy: {test_accuracy / nb_test_steps}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on a sample text\n",
        "\n",
        "text = datax[1]['text']\n",
        "# Tokenize the text\n",
        "\n",
        "def extract_keywords(text,tokenizer, model):\n",
        "  tokenized_text = tokenizer(text, padding=True, truncation=True, max_length=128*2, return_tensors='pt')\n",
        "\n",
        "  # Make a prediction\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(tokenized_text['input_ids'].to(device), tokenized_text['attention_mask'].to(device))\n",
        "\n",
        "      # Get the predicted labels\n",
        "\n",
        "      preds = torch.argmax(outputs.logits, dim=2).flatten()\n",
        "      # Get the predicted keywords\n",
        "\n",
        "      keywords = [tokenizer.decode(tokenized_text['input_ids'][0][i]) for i, pred in enumerate(preds) if pred == 1]\n",
        "\n",
        "      return keywords\n",
        "\n",
        "text = datax[1]['text']\n",
        "\n",
        "print(extract_keywords(text,tokenizer, model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi-s_a4m3Np7",
        "outputId": "f37e83d4-6f79-48ae-aef5-03e02791e25b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['compte']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "\n",
        "model_name = 'camembert-keywords-finetuned'\n",
        "model.save_pretrained(model_name)\n",
        "tokenizer.save_pretrained(model_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkxDPhII-khb",
        "outputId": "4fdb0354-d8c9-415e-852c-40cab915f5f2"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('camembert-keywords-finetuned/tokenizer_config.json',\n",
              " 'camembert-keywords-finetuned/special_tokens_map.json',\n",
              " 'camembert-keywords-finetuned/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model\n",
        "\n",
        "\n",
        "model2 = CamembertForTokenClassification2.from_pretrained(model_name)\n",
        "model2.to(\"cuda\")\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Extract \n",
        "\n",
        "print(extract_keywords(text,tokenizer2, model2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpyzhCdA_D-h",
        "outputId": "e9abf412-88ff-46c4-daf0-24a2211a5359"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['compte']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize the model\n",
        "\n",
        "model_int8 = torch.ao.quantization.quantize_dynamic(\n",
        "    model2,  # the original model\n",
        "    {torch.nn.Linear},  # a set of layers to dynamically quantize\n",
        "    dtype=torch.qint8)  # the target dtype for quantized weights\n",
        "\n",
        "\n",
        "# Save the quantized model\n",
        "model_int8_tok = 'camembert-keywords-finetuned-quantized'\n",
        "model_int8_name = model_int8_tok+'/pytorch_model.bin'\n",
        "torch.save(model_int8.state_dict(), model_int8_name)\n",
        "tokenizer2.save_pretrained(model_int8_tok)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyGSpDoJBK3Y",
        "outputId": "5f3cc5f9-8c19-4c1b-b6c2-c766d302afa0"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('camembert-keywords-finetuned-quantized/tokenizer_config.json',\n",
              " 'camembert-keywords-finetuned-quantized/special_tokens_map.json',\n",
              " 'camembert-keywords-finetuned-quantized/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the quantized model\n",
        "model_int8_tok = 'camembert-keywords-finetuned-quantized'\n",
        "model_name = 'camembert-keywords-finetuned'\n",
        "\n",
        "loaded_model = CamembertForTokenClassification2.from_pretrained(model_int8_tok)\n",
        "loaded_state_dict = torch.load(model_int8_name)\n",
        "loaded_model.load_state_dict(loaded_state_dict,strict=False)\n",
        "loaded_model.to('cuda')\n",
        "\n",
        "tokenizer3 = AutoTokenizer.from_pretrained(model_int8_tok)\n",
        "\n",
        "# Extract keywords\n",
        "\n",
        "print(extract_keywords(text,tokenizer3, loaded_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "did6sVSdBe-p",
        "outputId": "41d91da1-d797-49ea-9ed3-7623ab1e82b7"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:314: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  device=storage.device,\n",
            "Some weights of the model checkpoint at camembert-keywords-finetuned-quantized were not used when initializing CamembertForTokenClassification2: ['model.roberta.encoder.layer.6.intermediate.dense._packed_params.dtype', 'model.roberta.encoder.layer.5.attention.self.key.zero_point', 'model.roberta.encoder.layer.11.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.0.attention.self.key._packed_params._packed_params', 'model.roberta.encoder.layer.2.attention.self.query._packed_params._packed_params', 'model.roberta.encoder.layer.6.attention.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.3.intermediate.dense.zero_point', 'model.roberta.encoder.layer.5.attention.self.query._packed_params._packed_params', 'model.roberta.encoder.layer.7.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.7.attention.output.dense.scale', 'model.roberta.encoder.layer.7.attention.self.value.scale', 'model.roberta.encoder.layer.5.intermediate.dense._packed_params._packed_params', 'model.roberta.encoder.layer.10.intermediate.dense._packed_params.dtype', 'model.roberta.encoder.layer.9.intermediate.dense.zero_point', 'classifier.scale', 'model.roberta.encoder.layer.9.attention.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.5.attention.self.value.scale', 'model.roberta.encoder.layer.3.attention.self.value._packed_params._packed_params', 'model.roberta.encoder.layer.11.attention.self.query.zero_point', 'model.roberta.encoder.layer.4.attention.self.query.scale', 'model.roberta.encoder.layer.11.attention.self.query._packed_params._packed_params', 'model.roberta.encoder.layer.10.attention.self.key.scale', 'model.roberta.encoder.layer.7.attention.self.key.scale', 'model.roberta.encoder.layer.0.attention.self.key.scale', 'model.roberta.encoder.layer.4.attention.self.query.zero_point', 'model.roberta.encoder.layer.10.attention.self.value._packed_params.dtype', 'model.roberta.encoder.layer.1.attention.self.value._packed_params._packed_params', 'model.roberta.encoder.layer.3.output.dense.scale', 'model.roberta.encoder.layer.0.attention.self.value.scale', 'model.roberta.encoder.layer.10.attention.self.query._packed_params.dtype', 'model.roberta.encoder.layer.9.output.dense.scale', 'model.roberta.encoder.layer.0.output.dense.zero_point', 'model.roberta.encoder.layer.6.intermediate.dense._packed_params._packed_params', 'model.roberta.encoder.layer.9.attention.self.query.zero_point', 'model.roberta.encoder.layer.5.attention.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.11.attention.self.value.scale', 'model.roberta.encoder.layer.1.attention.self.key._packed_params._packed_params', 'model.roberta.encoder.layer.9.intermediate.dense._packed_params.dtype', 'model.roberta.encoder.layer.4.intermediate.dense._packed_params.dtype', 'model.roberta.encoder.layer.0.attention.self.value._packed_params.dtype', 'model.roberta.encoder.layer.6.attention.self.query._packed_params._packed_params', 'model.roberta.encoder.layer.2.attention.self.key.zero_point', 'model.roberta.encoder.layer.1.attention.self.key.scale', 'model.roberta.encoder.layer.2.attention.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.9.attention.output.dense.zero_point', 'model.roberta.encoder.layer.6.attention.self.query._packed_params.dtype', 'model.roberta.encoder.layer.7.output.dense.scale', 'model.roberta.encoder.layer.6.attention.self.key.scale', 'model.roberta.encoder.layer.2.attention.self.key._packed_params._packed_params', 'model.roberta.encoder.layer.5.attention.self.value._packed_params.dtype', 'model.roberta.encoder.layer.9.intermediate.dense.scale', 'model.roberta.encoder.layer.8.intermediate.dense._packed_params.dtype', 'model.roberta.encoder.layer.0.attention.self.query._packed_params.dtype', 'model.roberta.encoder.layer.9.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.5.output.dense.scale', 'model.roberta.encoder.layer.8.attention.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.9.attention.self.value._packed_params.dtype', 'model.roberta.encoder.layer.1.attention.self.query.zero_point', 'model.roberta.encoder.layer.0.attention.self.query.zero_point', 'model.roberta.encoder.layer.10.attention.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.4.attention.self.value.zero_point', 'model.roberta.encoder.layer.8.attention.self.query._packed_params.dtype', 'model.roberta.encoder.layer.4.attention.output.dense.zero_point', 'model.roberta.encoder.layer.5.attention.self.key._packed_params._packed_params', 'model.roberta.encoder.layer.11.attention.self.query.scale', 'model.roberta.encoder.layer.3.intermediate.dense.scale', 'model.roberta.encoder.layer.6.attention.self.key._packed_params._packed_params', 'model.roberta.encoder.layer.1.attention.self.value.zero_point', 'model.roberta.encoder.layer.4.attention.self.key._packed_params.dtype', 'model.roberta.encoder.layer.2.attention.self.value._packed_params.dtype', 'model.roberta.encoder.layer.10.attention.self.key._packed_params._packed_params', 'model.roberta.encoder.layer.9.attention.self.key._packed_params._packed_params', 'model.roberta.encoder.layer.3.intermediate.dense._packed_params._packed_params', 'model.roberta.encoder.layer.3.intermediate.dense._packed_params.dtype', 'model.roberta.encoder.layer.3.attention.self.value.zero_point', 'model.roberta.encoder.layer.7.attention.self.query.zero_point', 'model.roberta.encoder.layer.10.attention.output.dense.zero_point', 'model.roberta.encoder.layer.10.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.4.output.dense.zero_point', 'model.roberta.encoder.layer.11.attention.output.dense.zero_point', 'model.roberta.encoder.layer.9.attention.self.key.scale', 'model.roberta.encoder.layer.7.attention.self.query._packed_params.dtype', 'model.roberta.encoder.layer.10.attention.self.query.zero_point', 'model.roberta.encoder.layer.10.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.1.intermediate.dense.scale', 'model.roberta.encoder.layer.5.attention.output.dense.scale', 'model.roberta.encoder.layer.10.attention.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.11.attention.self.key.zero_point', 'model.roberta.encoder.layer.4.attention.self.query._packed_params._packed_params', 'model.roberta.encoder.layer.11.intermediate.dense.zero_point', 'model.roberta.encoder.layer.3.attention.self.value._packed_params.dtype', 'model.roberta.encoder.layer.9.attention.self.value.scale', 'model.roberta.encoder.layer.6.attention.output.dense.scale', 'model.roberta.encoder.layer.9.intermediate.dense._packed_params._packed_params', 'model.roberta.encoder.layer.4.attention.self.value._packed_params._packed_params', 'model.roberta.encoder.layer.1.attention.self.value.scale', 'model.roberta.encoder.layer.10.attention.self.value.scale', 'classifier.zero_point', 'model.roberta.encoder.layer.0.attention.self.value.zero_point', 'model.roberta.encoder.layer.1.output.dense.zero_point', 'model.roberta.encoder.layer.9.attention.self.query._packed_params.dtype', 'model.roberta.encoder.layer.11.intermediate.dense._packed_params.dtype', 'model.roberta.encoder.layer.6.attention.self.key._packed_params.dtype', 'model.roberta.encoder.layer.6.attention.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.8.output.dense.scale', 'model.roberta.encoder.layer.1.output.dense.scale', 'model.roberta.encoder.layer.5.attention.self.value.zero_point', 'model.roberta.encoder.layer.10.attention.self.value._packed_params._packed_params', 'model.roberta.encoder.layer.8.attention.self.value.scale', 'model.roberta.encoder.layer.10.intermediate.dense.zero_point', 'model.roberta.encoder.layer.3.attention.self.query._packed_params._packed_params', 'model.roberta.encoder.layer.0.attention.self.value._packed_params._packed_params', 'model.roberta.encoder.layer.6.attention.self.value.zero_point', 'model.roberta.encoder.layer.10.attention.self.value.zero_point', 'model.roberta.encoder.layer.7.attention.self.query.scale', 'model.roberta.encoder.layer.6.output.dense.scale', 'model.roberta.encoder.layer.2.attention.self.value.scale', 'model.roberta.encoder.layer.3.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.7.attention.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.9.attention.self.value._packed_params._packed_params', 'model.roberta.encoder.layer.0.attention.self.key.zero_point', 'model.roberta.encoder.layer.3.attention.self.key._packed_params.dtype', 'model.roberta.encoder.layer.1.attention.self.value._packed_params.dtype', 'model.roberta.encoder.layer.0.output.dense.scale', 'model.roberta.encoder.layer.8.attention.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.6.attention.self.key.zero_point', 'model.roberta.encoder.layer.7.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.2.output.dense.scale', 'model.roberta.encoder.layer.7.output.dense.zero_point', 'model.roberta.encoder.layer.10.attention.self.key.zero_point', 'model.roberta.encoder.layer.11.output.dense.scale', 'model.roberta.encoder.layer.3.attention.self.query.scale', 'model.roberta.encoder.layer.11.attention.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.5.intermediate.dense.scale', 'model.roberta.encoder.layer.0.intermediate.dense.scale', 'model.roberta.encoder.layer.4.output.dense.scale', 'model.roberta.encoder.layer.1.intermediate.dense._packed_params._packed_params', 'model.roberta.encoder.layer.8.attention.self.query.scale', 'model.roberta.encoder.layer.4.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.0.attention.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.9.attention.self.value.zero_point', 'model.roberta.encoder.layer.2.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.8.attention.self.value.zero_point', 'model.roberta.encoder.layer.11.output.dense.zero_point', 'model.roberta.encoder.layer.6.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.8.intermediate.dense.scale', 'model.roberta.encoder.layer.7.attention.self.value.zero_point', 'model.roberta.encoder.layer.8.output.dense.zero_point', 'model.roberta.encoder.layer.6.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.10.attention.self.key._packed_params.dtype', 'model.roberta.encoder.layer.1.attention.self.query._packed_params._packed_params', 'model.roberta.encoder.layer.6.attention.self.query.zero_point', 'model.roberta.encoder.layer.10.output.dense.scale', 'model.roberta.encoder.layer.5.intermediate.dense._packed_params.dtype', 'model.roberta.encoder.layer.7.attention.self.value._packed_params.dtype', 'model.roberta.encoder.layer.5.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.5.attention.self.query._packed_params.dtype', 'model.roberta.encoder.layer.7.intermediate.dense._packed_params.dtype', 'model.roberta.encoder.layer.8.attention.self.query._packed_params._packed_params', 'model.roberta.encoder.layer.7.intermediate.dense._packed_params._packed_params', 'model.roberta.encoder.layer.5.output.dense.zero_point', 'model.roberta.encoder.layer.7.attention.output.dense.zero_point', 'model.roberta.encoder.layer.9.attention.self.key.zero_point', 'model.roberta.encoder.layer.7.attention.self.query._packed_params._packed_params', 'model.roberta.encoder.layer.4.attention.self.key.scale', 'model.roberta.encoder.layer.11.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.1.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.4.attention.self.value._packed_params.dtype', 'model.roberta.encoder.layer.2.attention.self.query._packed_params.dtype', 'model.roberta.encoder.layer.4.attention.self.value.scale', 'model.roberta.encoder.layer.8.attention.self.value._packed_params.dtype', 'model.roberta.encoder.layer.11.attention.self.key.scale', 'model.roberta.encoder.layer.4.intermediate.dense.zero_point', 'model.roberta.encoder.layer.5.attention.self.key._packed_params.dtype', 'model.classifier._packed_params.dtype', 'model.roberta.encoder.layer.2.intermediate.dense.zero_point', 'model.roberta.encoder.layer.1.attention.self.key._packed_params.dtype', 'model.roberta.encoder.layer.3.attention.self.query.zero_point', 'model.roberta.encoder.layer.6.intermediate.dense.scale', 'model.roberta.encoder.layer.0.intermediate.dense._packed_params._packed_params', 'model.roberta.encoder.layer.2.attention.self.value._packed_params._packed_params', 'model.roberta.encoder.layer.4.attention.self.key.zero_point', 'model.roberta.encoder.layer.2.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.2.attention.self.value.zero_point', 'model.roberta.encoder.layer.9.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.8.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.8.attention.self.query.zero_point', 'model.roberta.encoder.layer.0.intermediate.dense.zero_point', 'model.roberta.encoder.layer.11.attention.output.dense.scale', 'model.roberta.encoder.layer.0.attention.output.dense.scale', 'model.roberta.encoder.layer.8.attention.output.dense.zero_point', 'model.roberta.encoder.layer.11.intermediate.dense._packed_params._packed_params', 'model.roberta.encoder.layer.8.intermediate.dense.zero_point', 'model.roberta.encoder.layer.4.attention.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.2.attention.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.7.attention.self.key._packed_params.dtype', 'model.roberta.encoder.layer.0.attention.self.query.scale', 'model.roberta.encoder.layer.3.attention.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.8.attention.self.key._packed_params.dtype', 'model.roberta.encoder.layer.10.attention.output.dense.scale', 'model.roberta.encoder.layer.1.attention.self.query.scale', 'model.roberta.encoder.layer.5.attention.output.dense.zero_point', 'model.roberta.encoder.layer.2.attention.output.dense.zero_point', 'model.roberta.encoder.layer.3.attention.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.2.output.dense.zero_point', 'model.roberta.encoder.layer.11.attention.self.key._packed_params.dtype', 'classifier._packed_params.dtype', 'model.roberta.encoder.layer.7.intermediate.dense.zero_point', 'model.roberta.encoder.layer.8.attention.self.key.zero_point', 'model.roberta.encoder.layer.7.attention.self.key.zero_point', 'model.classifier.zero_point', 'model.roberta.encoder.layer.11.attention.self.query._packed_params.dtype', 'model.roberta.encoder.layer.0.attention.self.query._packed_params._packed_params', 'model.roberta.encoder.layer.11.intermediate.dense.scale', 'model.roberta.encoder.layer.8.attention.output.dense.scale', 'model.roberta.encoder.layer.7.attention.self.key._packed_params._packed_params', 'model.roberta.encoder.layer.6.attention.self.value.scale', 'model.roberta.encoder.layer.0.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.10.attention.self.query.scale', 'model.roberta.encoder.layer.9.attention.self.key._packed_params.dtype', 'model.roberta.encoder.layer.6.intermediate.dense.zero_point', 'model.roberta.encoder.layer.9.output.dense.zero_point', 'model.roberta.encoder.layer.3.attention.self.key.scale', 'classifier._packed_params._packed_params', 'model.roberta.encoder.layer.5.attention.self.key.scale', 'model.roberta.encoder.layer.8.attention.self.value._packed_params._packed_params', 'model.roberta.encoder.layer.9.attention.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.0.attention.output.dense.zero_point', 'model.roberta.encoder.layer.3.attention.self.key._packed_params._packed_params', 'model.roberta.encoder.layer.8.attention.self.key.scale', 'model.roberta.encoder.layer.6.attention.output.dense.zero_point', 'model.roberta.encoder.layer.4.attention.self.key._packed_params._packed_params', 'model.roberta.encoder.layer.5.attention.self.query.scale', 'model.roberta.encoder.layer.11.attention.self.value._packed_params.dtype', 'model.roberta.encoder.layer.1.attention.output.dense.zero_point', 'model.roberta.encoder.layer.2.attention.self.key.scale', 'model.roberta.encoder.layer.2.attention.self.query.zero_point', 'model.roberta.encoder.layer.11.attention.output.dense._packed_params._packed_params', 'model.classifier.scale', 'model.roberta.encoder.layer.4.intermediate.dense.scale', 'model.roberta.encoder.layer.7.intermediate.dense.scale', 'model.roberta.encoder.layer.11.attention.self.value.zero_point', 'model.roberta.encoder.layer.3.attention.output.dense.scale', 'model.roberta.encoder.layer.1.attention.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.3.attention.output.dense.zero_point', 'model.roberta.encoder.layer.11.attention.self.key._packed_params._packed_params', 'model.roberta.encoder.layer.11.attention.self.value._packed_params._packed_params', 'model.roberta.encoder.layer.6.attention.self.value._packed_params._packed_params', 'model.roberta.encoder.layer.5.attention.self.value._packed_params._packed_params', 'model.roberta.encoder.layer.7.attention.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.0.attention.self.key._packed_params.dtype', 'model.roberta.encoder.layer.9.attention.self.query._packed_params._packed_params', 'model.roberta.encoder.layer.10.intermediate.dense._packed_params._packed_params', 'model.roberta.encoder.layer.1.attention.self.query._packed_params.dtype', 'model.roberta.encoder.layer.9.attention.output.dense.scale', 'model.roberta.encoder.layer.6.attention.self.query.scale', 'model.roberta.encoder.layer.9.attention.self.query.scale', 'model.roberta.encoder.layer.0.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.2.attention.output.dense.scale', 'model.roberta.encoder.layer.6.attention.self.value._packed_params.dtype', 'model.roberta.encoder.layer.3.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.7.attention.self.value._packed_params._packed_params', 'model.roberta.encoder.layer.10.output.dense.zero_point', 'model.roberta.encoder.layer.0.attention.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.5.intermediate.dense.zero_point', 'model.roberta.encoder.layer.10.intermediate.dense.scale', 'model.roberta.encoder.layer.1.attention.self.key.zero_point', 'model.roberta.encoder.layer.4.intermediate.dense._packed_params._packed_params', 'model.roberta.encoder.layer.2.attention.self.key._packed_params.dtype', 'model.roberta.encoder.layer.4.attention.self.query._packed_params.dtype', 'model.roberta.encoder.layer.1.intermediate.dense._packed_params.dtype', 'model.classifier._packed_params._packed_params', 'model.roberta.encoder.layer.10.attention.self.query._packed_params._packed_params', 'model.roberta.encoder.layer.1.intermediate.dense.zero_point', 'model.roberta.encoder.layer.5.attention.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.4.attention.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.6.output.dense.zero_point', 'model.roberta.encoder.layer.8.attention.self.key._packed_params._packed_params', 'model.roberta.encoder.layer.2.intermediate.dense._packed_params.dtype', 'model.roberta.encoder.layer.4.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.1.attention.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.2.intermediate.dense.scale', 'model.roberta.encoder.layer.4.attention.output.dense.scale', 'model.roberta.encoder.layer.3.attention.self.value.scale', 'model.roberta.encoder.layer.3.output.dense.zero_point', 'model.roberta.encoder.layer.5.output.dense._packed_params.dtype', 'model.roberta.encoder.layer.0.intermediate.dense._packed_params.dtype', 'model.roberta.encoder.layer.1.attention.output.dense.scale', 'model.roberta.encoder.layer.1.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.8.intermediate.dense._packed_params._packed_params', 'model.roberta.encoder.layer.5.attention.self.query.zero_point', 'model.roberta.encoder.layer.8.output.dense._packed_params._packed_params', 'model.roberta.encoder.layer.2.intermediate.dense._packed_params._packed_params', 'model.roberta.encoder.layer.3.attention.self.key.zero_point', 'model.roberta.encoder.layer.3.attention.self.query._packed_params.dtype', 'model.roberta.encoder.layer.2.attention.self.query.scale']\n",
            "- This IS expected if you are initializing CamembertForTokenClassification2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CamembertForTokenClassification2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of CamembertForTokenClassification2 were not initialized from the model checkpoint at camembert-keywords-finetuned-quantized and are newly initialized: ['model.roberta.encoder.layer.0.attention.self.key.bias', 'model.roberta.encoder.layer.0.intermediate.dense.bias', 'model.roberta.encoder.layer.7.attention.output.dense.bias', 'model.roberta.encoder.layer.11.attention.self.query.weight', 'model.roberta.encoder.layer.11.attention.self.value.weight', 'model.roberta.encoder.layer.9.output.dense.bias', 'model.roberta.encoder.layer.1.attention.self.value.weight', 'model.roberta.encoder.layer.11.attention.output.dense.weight', 'model.roberta.encoder.layer.8.intermediate.dense.bias', 'model.roberta.encoder.layer.4.output.dense.weight', 'model.roberta.encoder.layer.2.attention.output.dense.bias', 'model.roberta.encoder.layer.7.output.dense.bias', 'model.roberta.encoder.layer.11.attention.output.dense.bias', 'model.roberta.encoder.layer.1.attention.self.key.weight', 'model.roberta.encoder.layer.3.attention.output.dense.weight', 'model.roberta.encoder.layer.8.output.dense.weight', 'model.roberta.encoder.layer.7.intermediate.dense.bias', 'model.roberta.encoder.layer.1.attention.self.value.bias', 'model.roberta.encoder.layer.2.intermediate.dense.weight', 'model.roberta.encoder.layer.3.attention.self.query.bias', 'model.roberta.encoder.layer.11.intermediate.dense.weight', 'model.roberta.encoder.layer.4.attention.self.key.weight', 'model.roberta.encoder.layer.11.attention.self.key.weight', 'model.roberta.encoder.layer.4.attention.self.query.bias', 'model.roberta.encoder.layer.9.attention.self.value.weight', 'model.roberta.encoder.layer.5.attention.output.dense.bias', 'model.roberta.encoder.layer.6.attention.output.dense.weight', 'model.roberta.encoder.layer.8.attention.self.key.bias', 'classifier.bias', 'model.roberta.encoder.layer.1.attention.output.dense.weight', 'model.roberta.encoder.layer.0.attention.self.query.bias', 'model.roberta.encoder.layer.2.intermediate.dense.bias', 'model.roberta.encoder.layer.7.attention.self.key.weight', 'model.roberta.encoder.layer.0.output.dense.weight', 'model.roberta.encoder.layer.1.attention.self.query.bias', 'model.roberta.encoder.layer.6.intermediate.dense.bias', 'model.roberta.encoder.layer.0.attention.output.dense.weight', 'model.roberta.encoder.layer.8.attention.self.query.bias', 'model.roberta.encoder.layer.7.attention.self.value.bias', 'model.roberta.encoder.layer.2.attention.output.dense.weight', 'model.roberta.encoder.layer.9.attention.self.key.bias', 'model.roberta.encoder.layer.2.output.dense.weight', 'model.roberta.encoder.layer.5.attention.self.key.weight', 'model.roberta.encoder.layer.1.attention.output.dense.bias', 'model.roberta.encoder.layer.11.attention.self.value.bias', 'model.roberta.encoder.layer.10.attention.self.query.weight', 'model.roberta.encoder.layer.4.attention.self.value.weight', 'model.roberta.encoder.layer.6.attention.self.query.weight', 'model.roberta.encoder.layer.4.attention.output.dense.bias', 'model.roberta.encoder.layer.8.attention.self.value.weight', 'model.roberta.encoder.layer.10.output.dense.weight', 'model.roberta.encoder.layer.10.attention.self.query.bias', 'model.roberta.encoder.layer.1.intermediate.dense.bias', 'model.roberta.encoder.layer.2.attention.self.key.bias', 'model.roberta.encoder.layer.9.attention.self.key.weight', 'model.roberta.encoder.layer.7.intermediate.dense.weight', 'model.roberta.encoder.layer.3.attention.self.value.weight', 'model.roberta.encoder.layer.11.intermediate.dense.bias', 'model.roberta.encoder.layer.4.attention.self.value.bias', 'model.roberta.encoder.layer.8.attention.self.query.weight', 'model.roberta.encoder.layer.4.attention.self.query.weight', 'model.roberta.encoder.layer.6.attention.self.key.bias', 'model.roberta.encoder.layer.5.attention.self.value.weight', 'model.roberta.encoder.layer.11.attention.self.key.bias', 'model.roberta.encoder.layer.7.attention.self.key.bias', 'model.classifier.bias', 'model.roberta.encoder.layer.10.attention.output.dense.bias', 'model.roberta.encoder.layer.9.intermediate.dense.weight', 'model.roberta.encoder.layer.9.attention.self.query.weight', 'model.roberta.encoder.layer.2.attention.self.key.weight', 'model.roberta.encoder.layer.6.attention.output.dense.bias', 'model.roberta.encoder.layer.0.attention.output.dense.bias', 'model.roberta.encoder.layer.10.attention.self.key.bias', 'model.roberta.encoder.layer.5.attention.self.value.bias', 'model.roberta.encoder.layer.1.output.dense.weight', 'model.roberta.encoder.layer.3.output.dense.weight', 'model.roberta.encoder.layer.5.attention.output.dense.weight', 'model.roberta.encoder.layer.5.intermediate.dense.bias', 'model.roberta.encoder.layer.6.attention.self.value.bias', 'model.roberta.encoder.layer.9.attention.self.query.bias', 'model.roberta.encoder.layer.3.intermediate.dense.weight', 'model.roberta.encoder.layer.8.intermediate.dense.weight', 'model.roberta.encoder.layer.3.output.dense.bias', 'model.roberta.encoder.layer.9.intermediate.dense.bias', 'model.roberta.encoder.layer.9.attention.output.dense.bias', 'model.roberta.encoder.layer.7.output.dense.weight', 'model.roberta.encoder.layer.3.attention.output.dense.bias', 'model.roberta.encoder.layer.10.attention.self.value.weight', 'model.roberta.encoder.layer.10.intermediate.dense.weight', 'model.roberta.encoder.layer.1.intermediate.dense.weight', 'model.roberta.encoder.layer.2.attention.self.query.weight', 'model.roberta.encoder.layer.2.attention.self.query.bias', 'model.roberta.encoder.layer.6.intermediate.dense.weight', 'model.roberta.encoder.layer.6.output.dense.weight', 'model.roberta.encoder.layer.6.output.dense.bias', 'model.roberta.encoder.layer.8.attention.output.dense.bias', 'model.roberta.encoder.layer.4.attention.self.key.bias', 'model.roberta.encoder.layer.7.attention.output.dense.weight', 'model.roberta.encoder.layer.5.output.dense.weight', 'model.roberta.encoder.layer.7.attention.self.value.weight', 'model.roberta.encoder.layer.10.attention.self.value.bias', 'model.roberta.encoder.layer.0.attention.self.value.weight', 'model.roberta.encoder.layer.6.attention.self.key.weight', 'model.roberta.encoder.layer.7.attention.self.query.bias', 'model.roberta.encoder.layer.8.attention.output.dense.weight', 'model.roberta.encoder.layer.4.attention.output.dense.weight', 'model.roberta.encoder.layer.9.output.dense.weight', 'model.roberta.encoder.layer.1.output.dense.bias', 'model.roberta.encoder.layer.8.attention.self.value.bias', 'model.roberta.encoder.layer.11.output.dense.weight', 'model.roberta.encoder.layer.0.attention.self.value.bias', 'model.roberta.encoder.layer.9.attention.output.dense.weight', 'model.roberta.encoder.layer.10.attention.self.key.weight', 'classifier.weight', 'model.roberta.encoder.layer.11.attention.self.query.bias', 'model.roberta.encoder.layer.3.attention.self.key.weight', 'model.roberta.encoder.layer.2.attention.self.value.bias', 'model.roberta.encoder.layer.5.output.dense.bias', 'model.roberta.encoder.layer.0.attention.self.key.weight', 'model.roberta.encoder.layer.3.attention.self.value.bias', 'model.roberta.encoder.layer.10.intermediate.dense.bias', 'model.roberta.encoder.layer.5.attention.self.key.bias', 'model.roberta.encoder.layer.3.attention.self.key.bias', 'model.classifier.weight', 'model.roberta.encoder.layer.2.attention.self.value.weight', 'model.roberta.encoder.layer.7.attention.self.query.weight', 'model.roberta.encoder.layer.2.output.dense.bias', 'model.roberta.encoder.layer.6.attention.self.query.bias', 'model.roberta.encoder.layer.8.output.dense.bias', 'model.roberta.encoder.layer.0.intermediate.dense.weight', 'model.roberta.encoder.layer.10.attention.output.dense.weight', 'model.roberta.encoder.layer.9.attention.self.value.bias', 'model.roberta.encoder.layer.8.attention.self.key.weight', 'model.roberta.encoder.layer.5.intermediate.dense.weight', 'model.roberta.encoder.layer.10.output.dense.bias', 'model.roberta.encoder.layer.1.attention.self.query.weight', 'model.roberta.encoder.layer.3.intermediate.dense.bias', 'model.roberta.encoder.layer.4.intermediate.dense.bias', 'model.roberta.encoder.layer.11.output.dense.bias', 'model.roberta.encoder.layer.3.attention.self.query.weight', 'model.roberta.encoder.layer.0.output.dense.bias', 'model.roberta.encoder.layer.6.attention.self.value.weight', 'model.roberta.encoder.layer.4.intermediate.dense.weight', 'model.roberta.encoder.layer.4.output.dense.bias', 'model.roberta.encoder.layer.1.attention.self.key.bias', 'model.roberta.encoder.layer.5.attention.self.query.bias', 'model.roberta.encoder.layer.0.attention.self.query.weight', 'model.roberta.encoder.layer.5.attention.self.query.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['entreprise']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(modelx):\n",
        "  test_loss, test_accuracy = 0, 0\n",
        "  nb_test_steps, nb_test_examples = 0, 0\n",
        "\n",
        "  for batch in val_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "      with torch.no_grad():\n",
        "          outputs = modelx(input_ids=b_input_ids, \n",
        "                      attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "      loss = outputs.loss\n",
        "      logits = outputs.logits\n",
        "      if loss is not None:\n",
        "        test_loss += loss.item()\n",
        "\n",
        "      test_accuracy += flat_accuracy(logits, b_labels)\n",
        "\n",
        "\n",
        "      nb_test_examples += b_input_ids.size(0)\n",
        "      nb_test_steps += 1\n",
        "\n",
        "\n",
        "  print(f'Validation loss: {test_loss / nb_test_steps}')\n",
        "  print(f'Validation accuracy: {test_accuracy / nb_test_steps}')"
      ],
      "metadata": {
        "id": "mPoKc66xH8s7"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(loaded_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giOkIIlFIUue",
        "outputId": "c6236a8c-4aaf-4a78-845e-5ed779dd326e"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-74-f54ee28d1cc3>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_ids_tensor[i, :len(ids)] = torch.tensor(ids)\n",
            "<ipython-input-74-f54ee28d1cc3>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels_tensor[i, :len(ids)] = torch.tensor(labels[i])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.2880912125110626\n",
            "Validation accuracy: 0.9240261912345886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"camembert-keywords-finetuned-quantized\", 'zip', \"camembert-keywords-finetuned-quantized\")\n",
        "shutil.make_archive(\"camembert-keywords-finetuned\", 'zip', \"camembert-keywords-finetuned\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ic3szqmMNIj4",
        "outputId": "2c8db19a-c330-4015-9ac3-17098fc53d83"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/camembert-keywords-finetuned.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"camembert-keywords-finetuned-quantized.zip\")\n",
        "files.download(\"camembert-keywords-finetuned.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "hKSUREqGNulI",
        "outputId": "c2afd079-e11c-4b26-8e04-38cb6dcc845f"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_50df5432-38e8-45b3-a918-b8667cf42df3\", \"camembert-keywords-finetuned-quantized.zip\", 139469213)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d7423c7d-0421-47d0-9ba0-a2e7f8c5dc06\", \"camembert-keywords-finetuned.zip\", 408002277)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}