{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch transformers pytorch-lightning datasets seaborn plotly pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import functools\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from transformers import AutoModelForSequenceClassification, CamembertForMaskedLM, AutoTokenizer, AutoConfig\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train samples : 49127\n",
      "Total validation samples: 1988\n",
      "Total test samples: 2000\n"
     ]
    }
   ],
   "source": [
    "dataset = \"data/PAWS-C-FR/\"\n",
    "data = {\n",
    "    \"train\": dataset + \"translated_train.tsv\",\n",
    "    \"dev\": dataset + \"dev_2k.tsv\",\n",
    "    \"test\": dataset + \"test_2k.tsv\"\n",
    "}\n",
    "\n",
    "\n",
    "def load_process():\n",
    "    train = pd.read_csv(data['train'], delimiter='\\t', on_bad_lines='skip')\n",
    "    dev = pd.read_csv(data['dev'], delimiter='\\t', on_bad_lines='skip')\n",
    "    test = pd.read_csv (data['test'], delimiter='\\t', on_bad_lines='skip')\n",
    "\n",
    "    train.drop(columns=['id'], inplace=True)\n",
    "    dev.drop(columns=['id'], inplace=True)\n",
    "    test.drop(columns=['id'], inplace=True)\n",
    "\n",
    "    train.dropna(inplace=True)\n",
    "    dev.dropna(inplace=True)\n",
    "    test.dropna(inplace=True)\n",
    "\n",
    "    train['label'] = train['label'].astype(int)\n",
    "    dev['label'] = dev['label'].astype(int)\n",
    "    test['label'] = test['label'].astype(int)\n",
    "\n",
    "    return train, dev, test\n",
    "\n",
    "\n",
    "train, dev, test = load_process()\n",
    "\n",
    "# Shape of the data\n",
    "print(f\"Total train samples : {train.shape[0]}\")\n",
    "print(f\"Total validation samples: {dev.shape[0]}\")\n",
    "print(f\"Total test samples: {test.shape[0]}\")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "dev_dataset = Dataset.from_pandas(dev)\n",
    "test_dataset = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "def tokenize_batch(samples, tokenizer):\n",
    "    sentence_1 = [sample['sentence1'] for sample in samples]\n",
    "    sentence_2 = [sample['sentence2'] for sample in samples]\n",
    "    labels = torch.tensor([sample[\"label\"] for sample in samples])\n",
    "    str_labels = [sample[\"label\"] for sample in samples]\n",
    "    text = [[str(x), str(y)] for x,y in zip(sentence_1, sentence_2)]\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", padding='max_length', max_length = 128, truncation=True)\n",
    "\n",
    "    return {\"input_ids\": tokens.input_ids, \"attention_mask\": tokens.attention_mask, \"labels\": labels, \"str_labels\": str_labels, \"sentences\": text}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=True, \n",
    "    num_workers=12,\n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer)\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dev_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    num_workers=12,\n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/abdessalem/Desktop/Projects/edu-keywords-extraction/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_labels, lr, weight_decay, from_scratch=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        if from_scratch:\n",
    "            config = AutoConfig.from_pretrained(\n",
    "                model_name, num_labels=num_labels\n",
    "            )\n",
    "            self.model = AutoModelForSequenceClassification.from_config(config)\n",
    "        else:\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_name, num_labels=num_labels\n",
    "            )\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_labels = self.model.num_labels\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        logits = out.logits\n",
    "        # -------- MASKED --------\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits.view(-1, self.num_labels), batch[\"labels\"].view(-1))\n",
    "\n",
    "        # ------ END MASKED ------\n",
    "\n",
    "        self.log(\"train/loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        labels = batch[\"labels\"]\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        preds = torch.max(out.logits, -1).indices\n",
    "        # -------- MASKED --------\n",
    "        acc = (batch[\"labels\"] == preds).float().mean()\n",
    "        # ------ END MASKED ------\n",
    "        self.log(\"valid/acc\", acc)\n",
    "\n",
    "        f1 = f1_score(batch[\"labels\"].cpu().tolist(), preds.cpu().tolist(), average=\"macro\")\n",
    "        self.log(\"valid/f1\", f1)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        return torch.max(out.logits, -1).indices\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "lightning_model = LightningModel(\"camembert-base\", 2, lr=3e-5, weight_decay=0.)\n",
    "model_checkpoint = pl.callbacks.ModelCheckpoint(monitor=\"valid/acc\", mode=\"max\")\n",
    "\n",
    "camembert_trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\", devices=\"auto\",\n",
    "    callbacks=[\n",
    "        pl.callbacks.EarlyStopping(monitor=\"valid/acc\", patience=4, mode=\"max\"),\n",
    "        model_checkpoint,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                               | Params\n",
      "-------------------------------------------------------------\n",
      "0 | model | CamembertForSequenceClassification | 110 M \n",
      "-------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.494   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdessalem/Desktop/Projects/edu-keywords-extraction/venv/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 3071/3071 [33:17<00:00,  1.54it/s, v_num=2]      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdessalem/Desktop/Projects/edu-keywords-extraction/venv/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 3071/3071 [33:19<00:00,  1.54it/s, v_num=2]\n"
     ]
    }
   ],
   "source": [
    "camembert_trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model,trainer,test_set):\n",
    "    preds = trainer.predict(model,dataloaders=test_set)\n",
    "    preds = torch.cat(preds, -1)\n",
    "    preds = preds.cpu().numpy()\n",
    "    preds = preds.tolist()\n",
    "    test['preds'] = preds\n",
    "    test['preds'] = test['preds'].astype(int)\n",
    "    test['label'] = test['label'].astype(int)\n",
    "    print(f\"Accuracy: {sum(test['preds'] == test['label'])/len(test)}\")\n",
    "    print(f\"F1 score: {f1_score(test['preds'], test['label'], average='macro')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 125/125 [00:28<00:00,  4.42it/s]\n",
      "Accuracy: 0.906\n",
      "F1 score: 0.9056935985015315\n"
     ]
    }
   ],
   "source": [
    "lightning_model = LightningModel.load_from_checkpoint(checkpoint_path=model_checkpoint.best_model_path)\n",
    "\n",
    "eval(lightning_model,camembert_trainer,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the state dictionary of the model\n",
    "torch.save(lightning_model.state_dict(), \"camembert-prefinetuned.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LightningModel(\n",
       "  (model): CamembertForSequenceClassification(\n",
       "    (roberta): CamembertModel(\n",
       "      (embeddings): CamembertEmbeddings(\n",
       "        (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): CamembertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x CamembertLayer(\n",
       "            (attention): CamembertAttention(\n",
       "              (self): CamembertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): CamembertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): CamembertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): CamembertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): CamembertClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "loaded_model = LightningModel(\"camembert-base\", 2, lr=3e-5, weight_decay=0.)\n",
    "loaded_state_dict = torch.load(\"camembert-prefinetuned.pt\")\n",
    "loaded_model.load_state_dict(loaded_state_dict)\n",
    "loaded_model.to('cuda')\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 125/125 [00:28<00:00,  4.38it/s]\n",
      "Accuracy: 0.906\n",
      "F1 score: 0.9056935985015315\n"
     ]
    }
   ],
   "source": [
    "eval(loaded_model,camembert_trainer,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize model using dynamic quantization\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    loaded_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "torch.save(quantized_model.state_dict(), 'models/camembert-prefinetuned-quantized.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 442.567241\n",
      "Size (MB): 186.048485\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "print_size_of_model(loaded_model)\n",
    "print_size_of_model(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LightningModel(\n",
       "  (model): CamembertForSequenceClassification(\n",
       "    (roberta): CamembertModel(\n",
       "      (embeddings): CamembertEmbeddings(\n",
       "        (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): CamembertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x CamembertLayer(\n",
       "            (attention): CamembertAttention(\n",
       "              (self): CamembertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): CamembertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): CamembertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): CamembertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): CamembertClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model\n",
    "device = torch.device(\"cuda\")\n",
    "loaded_quantized_model = LightningModel(\"camembert-base\", 2, lr=3e-5, weight_decay=0.)\n",
    "loaded_quantized_state_dict = torch.load(\"models/camembert-prefinetuned-quantized.pt\",map_location=device)\n",
    "loaded_quantized_model.load_state_dict(loaded_quantized_state_dict, strict=False)\n",
    "loaded_quantized_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 125/125 [00:28<00:00,  4.38it/s]\n",
      "Accuracy: 0.453\n",
      "F1 score: 0.3253368690450495\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "loaded_quantized_model.eval()\n",
    "eval(loaded_quantized_model,camembert_trainer,test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
